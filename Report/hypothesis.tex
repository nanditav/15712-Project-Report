\section{Leveraging approximation to accelerate distribute training}
\subsection{Hypothesis}
In this project, we aim to test the hypothesis that different layers in a neural network require different amounts of training, and hence have varying tolerance towards approximations. Today, each of the layers in the neural networks, irrespective for their type or size, are trained for the same number of iterations. It's unclear, however, whether the same extent of training for all layers is required to reach the same convergence. A significant amount of training time is expended in calculating and backpropagating gradients through the different layers of DNN. Similarly, a large amount of data in the form of parameter updates are transferred across the network to ensure convergence of the model. If the aforementioned hypothesis is true, then a significant amount of training time and network bandwidth could be saved by reducing the number of times the some of the layers are trained{\textemdash}leading to better scability and shorter training time, without much loss in the model accuracy.  

\begin{itemize}
\item \textbf{Implication \#1:} Layers in the neural network have varying sensitivity towards approximation and asynchrony.
\item \textbf{Implication \#2:} Varying asynchrony and approximation over training time reducing training time while preserving convergence.
\end{itemize}
