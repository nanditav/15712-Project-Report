\section{Leveraging approximation to accelerate distribute training}
\subsection{Hypothesis}
In this project, we aim to test the hypothesis that different layers in a neural network require different amounts of training, and hence have varying tolerance towards approximations. Today, each of the layers in the neural networks, irrespective for their type or size, are trained for the same number of iterations. It's unclear, however, whether the same extent of training for all layers is required to reach the same convergence. A significant amount of training time is expended in calculating and backpropagating gradients through the different layers of DNN. Similarly, a large amount of data in the form of parameter updates are transferred across the network to ensure convergence of the model. If the aforementioned hypothesis is true, then a significant amount of training time and network bandwidth could be saved by reducing the number of times some of the layers are trained{\textemdash}leading to better scability and shorter training time, without much loss in the model accuracy. In the project, we aim to test the hypothesis that different layers require different amounts of training, and hence have varying tolerance towards approximation by evaluating two techniques that leverage heteregeneity in training requirements across layers:     
\begin{itemize}
\item \textbf{Technique \#1:} Different layers in the neural network can be trained and updated at \emph{different rates}.
\item \textbf{Technique \#2:} The training for several layers can be \emph{halted} at different points before the full completion of training. 
\end{itemize}
