\section{Evaluation and Analysis}
In this section, we evaluate the above techniques to leverage the approximation-tolerance of the neural network to accelerate distributed training. We essentially performing \textit{training update elisions}, which involves not updating the weights and biases of specific layers at certain training steps. Two kinds of training elisions were explored:
\begin{itemize}
	\item \textit{Periodic Training Elision} involves performing training elisions once every $n$ steps, where $n$ is called the periodic elision interval. The parameters of the entire network are updated for $n-1$ steps, followed by a step where only parameters corresponding to certain layers are updated.
	\item \textit{Selective Early Termination} involves training the entire network until a given training step $\kappa$. For steps after $\kappa$, only parameters corresponding to certain layers are updated, while updates and gradient computation for other layers are elided.

\end{itemize}

Both types of approximations partially update the model weights in a specified training step. We expect that using update elision will  reduce the amount of computation required in a training step, as well as the communication overhead required for distributed training. 

We now discuss the experimental results. Different approximation techniques are compared on the basis of classification accuracy (on the full testing data) and training time. We present results for both single node training as well as distributed training for a number of different cluster configurations. In each of our experiments, we checkpoint the model every 1000 trained steps, and use the checkpointed model to compute the achieved classification accuracy.

\subsection{Periodic Training Elision}
\subsubsection{Sensitivity of Different Layers to Training Elision}
To determine how the convergence of the model is impacted by the training elision for different layers, we conduct an experiment where we periodically do not train or update a different number of layers. Figure 1 depicts the accuracy achieved during training while periodically eliding training for a different number of layers starting from the bottom up (dashed lines) and top down (unbroken lines).
From Figure 1, we make the following observations: 
\begin{enumerate}
\item Eliding a single layer (top-most or bottom-most) has negligible impact on accuracy.
\item When eliding the \emph{same number} of layers, eliding training for the \emph{top layers} first consistently faster convergence and better accuracy than eliding the bottom layers.
\item Eliding the bottom 4 layers anomalously produces faster convergence than eliding the 3 bottom layers. 
\end{enumerate}
\subsubsection{Single Node Accuracy & Training Time.}
Figure 1: Periodic Training Elision (every 2 steps for both upper layers and lower layers) -- Accuracy
\textbf{How}
Figure 2: Periodic Training Elision (every 5) -- Accuracy
Figure 3: Periodic Training Elision (every 2 & 5 steps for both upper layers and lower layers) -- Execution Time
\subsubsection{Mutiple Workers Accuracy and Training Time.}
Figure 4: Periodic Training Elision 2 workers (every 2 steps for both upper layers and lower layers) -- Accuracy
Figure 5: Periodic Training Elision 4 workers (every 2 steps for both upper layers and lower layers) -- Accuracy
Figure 6: Periodic Training Elision 2&4 workers (every 2 steps for both upper layers and lower layers) -- Execution Time
\subsection{Selective Early Termination}
\subsubsection{Single Node.} 
Figure 7: Selective Early Termination (2K steps) -- Accuracy
Figure 8: Selective Early Termination (5K steps) -- Accuracy
Figure 9: Selective Early Termination (2K & 5K steps) -- Execution Time
\subsubsection{Multiple Workers}
Figure 10: Selective Early Termination 2 workers (5K steps) -- Accuracy 
Figure 11: Selective Early Termination 4 workers (2K steps) -- Accuracy
Figure 12: Selective Early Termination 2 & 4 (2K & 5K steps) -- Execution Time
\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\columnwidth]{figures/approx.eps}
	\caption{Selective Training Elision (after 5000 steps)}
	\label{fig:approx}
\end{figure}

In Figure \ref{fig:approx}, we plot the obtained accuracy versus the training step for a selective training elision experiment running on a single machine. For each of the compared experiments we train all layers of the model until the 5000th training step. For the subsequent steps the training data is used to update only the top $\eta$ trainable layers of the model, where $\eta$ was chosen between $1$ to $4$ in our experiments. Note that, for a $N$ layer neural network $\eta=i$ corresponds to update elision for the first $N-\eta$ trainable layers. From the plot we can observe that eliding layer updates has a slight impact on accuracy. The fewer the number of elided layers, the greater the accuracy of the trained model. 

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\columnwidth]{figures/periodic-approx-2.eps}
	\caption{Periodic Selective Training Elision (every 2 steps)}
	\label{fig:periodic-approx-2}
\end{figure}
\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\columnwidth]{figures/periodic-approx-5.eps}
	\caption{Periodic Selective Training Elision (every 5 steps)}
	\label{fig:periodic-approx-5}
\end{figure}

In Figures \ref{fig:periodic-approx-2} and \ref{fig:periodic-approx-5}, we plot the obtained accuracy versus the training step for periodic selective training elision experiments running on a single machine. For the experiments shown in Figure \ref{fig:periodic-approx-2}, the periodic elision interval $n$ was set to 2, and the those in Figure \ref{fig:periodic-approx-5}, $n$ was set to 5. 


\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\columnwidth]{figures/approx-2workers.eps}
	\caption{Selective Training Elision (after 5000 steps) using 2 Asynchronous Workers}
	\label{fig:approx-2-workers}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\columnwidth]{figures/combined_bottom_top_elisions.eps}
	\caption{Selective Training Elision (every 2 steps)}
	\label{fig:combined-alt}
\end{figure}
