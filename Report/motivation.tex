\section{Motivation} \label{sec:motivation}

In this section, we discuss the motivation for distributing
machine learning applications across different machines.
We then discuss our experiments to understand the effect of
synchrony on the performance and model accuracy of a distributed
deep neural network training task.

\subsection{Distributed Machine Learning}

Recent advances in AI tasks such as a speech recognition and visual 
object classification have been made possible by complex models
that operate on large amounts of data. Modern systems \cite{distbelief}
\cite{projectadam}\cite{parameterserver} develop large and complex
models with around a billion to trillion parameters. Handling data
at such scale is beyond the capability of any single machine. As a 
result, distributing the machine learning task across many different
machines has become a necessity. However, distributing work at 
this scale introduces new challenges.

Large scale distribution creates a communication bottleneck. Most 
machine learning applications are iterative algorithms that progressively
refine the result. The intuitive form of parallelism for such 
applications is to divide the input data among many workers and 
synchronize the progress made by each worker every iteration. The 
problem with this Bulk Synchronous Parallelism model of distribution 
is that the barrier at the end of each iteration affects scalability.
The presence of even a few stragglers (workers that take more time
to complete an iteration relative to other workers in a system) 
has a huge performance penalty on the application.

The problem of stragglers has spawned active research on a variety
of consistency models that relax synchronization without compromising
the convergence rate\cite{communicationthesis}\cite{ganger}.
We performed experiments to understand the effect of synchronization 
on the execution time and model accuracy of distributed training.

\subsection{Effect of Synchronization}

\begin{figure}[h]
\centering
  \includegraphics[keepaspectratio,width=\columnwidth]{figures/scaling_train_time.PNG}
  \caption{\textbf{Scalability to multiple workers}}
  \label{fig:scalability}
\end{figure}

Figure \ref{fig:scalability} shows the execution time required 
to complete 20000 steps when run with 1, 2, 4 and 6 workers with
and without synchronization. The 
execution times for all the configurations are normalized to 
the single worker case. The data shows poor scaling for distributed
training in both the synchronous and asynchronous modes. Increasing
the number of parameter servers also does not improve the scalability
of distributed training.
This performance improvement
comes at the cost of reduced accuracy of the trained network (~10\%
on average)

\begin{figure}[h]
\centering
  \includegraphics[keepaspectratio,width=\columnwidth]{figures/cpu_util.PNG}
  \caption{\textbf{CPU utilization across workers}}
  \label{fig:cpuutil}
\end{figure}

Figure \ref{fig:cpuutil} shows the average CPU utilization for the 
different configurations mentioned above. The data shows that the 
CPU utilization drops as the number of workers increases for 
{\em both} synchronous and asynchronous training. This trend
indicates the presence of stragglers for asynchronous training.

The poor scaling of distributed training motivates the need for
approximation strategies to improve execution time without 
significantly compromising model accuracy.

%% > Why distribute? 
%% Model parallelism allows training more complex models with many 
%% parameters. This is a progression from using GPUs to many different
%% machines. 
%% 
%% > synchronous training scalability. Impact of synchrony on accuracy
%% Why do we use synchronous training. Is it because we have a system
%% that is similar to the single node version? 
%% This berkeley thesis talks about why synchronization is important. 
%% Apparently synchronization allows for faster convergence because 
%% the average of the local updates of different machines reduces the
%% variance in data which leads to faster convergence. (https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-47.pdf)
%% Blindly removing synchrony in an application does not seem to be
%% completely useful.
%% 
%% The impact of asynchrony on accuracy is that we are making judgements
%% on only part of the data that was present in our local store. This
%% higlights the dependence of the distribution of data and the homogeniety
%% (in terms of information) of the data. I think this was the premise of 
%% the hogwild paper.
%% 
%% > How does asynchrony affect training time? And accuracy
%% Stragglers are an issue in any large scale distributed training. If 
%% we use machines with different processing times (as in a distributed
%% cluster with different processing times) then the effects of stragglers
%% are exaggerated. 




