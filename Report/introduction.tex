\section{Introduction}
Deep Neural Networks constitute a state-of-the-art technique in most domains which use machine learning. However, they are notoriously difficult to train and require significant computational resources. Traditional formulations of neural networks focus on serialized implementations, which in practice take a very long time to train. In order to overcome this shortcoming a lot of work has focused on parallelizing some aspects of the training phase in order to obtain speedup training. Single node platforms with GPUs have been one of the key ways by which system designers have been able to speed up training. However, a single system cannot be used for large scale training on petabytes of data. It is essential that the training be split up over multiple distributed nodes.

In this project we will focus on optimizing the training of deep neural networks from a systems perspective.
We will explore how approximation can help reduce training time without sacrificing accuracy. The project
is highly relevant to the course and would focus on a number of concepts covered in the course, such as
consistency and distributed computation models

