\section{Introduction}
Deep Neural Networks constitute a state-of-the-art technique across many domains which use machine learning such as autonomous cars, speech recognition, image/video processing, etc. These neural networks however require training over massively large datasets. Hence, training these networks has become notoriously difficult, requiring a significant amount of computational resources to complete in a reasonable amount of time. Traditional formulations of neural networks focus on serialized implementations, which in practice take a very long time to train. In order to overcome this shortcoming a lot of work has focused on parallelizing some aspects of the training phase in order to obtain speedup training. Single node platforms with GPUs have been one of the key ways by which system designers have been able to speed up training. However, a single system cannot be used for large scale training on petabytes of data. It is essential that the training be split up over multiple distributed nodes.

In this project we will focus on optimizing the training of deep neural networks from a systems perspective.
We will explore how approximation can help reduce training time without sacrificing accuracy. The project
is highly relevant to the course and would focus on a number of concepts covered in the course, such as
consistency and distributed computation models

