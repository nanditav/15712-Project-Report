\section{Introduction}
Deep Neural Networks constitute a state-of-the-art technique across many domains which use machine learning such as autonomous cars, speech recognition, image/video processing, etc. These neural networks however require training over massively large datasets. Hence, training these networks has become notoriously difficult, requiring a significant amount of computational resources to complete in a reasonable amount of time. Traditional formulations of neural networks focus on serialized implementations, which in practice take a very long time to train. In order to overcome this shortcoming a lot of work has focused on parallelizing some aspects of the training phase in order to obtain speedup training. Single node platforms with GPUs have been one of the key ways by which system designers have been able to speed up training. However, a single system cannot be used for large scale training on petabytes of data. It is essential that the training be split up over multiple distributed nodes. This has led to the development of large-scale distributed machine learning frameworks such as TensorFlow~\cite{tensorflow}. 

To test the scalability of distributed training, we performed a number of experiments using TensorFlow to perform the image classification task over the CIFAR-10~\cite{cifar10} dataset. Our experiments show that counterintuitively, scaling out across multiple does not provide the expected speedup in distributed training. A likely reason is the large amount of data that is exchanged over the network to keep the models that are trained in parallel consistent with each other, to ensure convergence. 

In this project, our goal is to accelerate distributed neural network training by leveraging the \emph{approximation-tolerant} nature of these algorithms. It is well known that neural networks are tolerant to towards different approximations that can be leveraged to speed up inference~\cite{x,y}, however, principled approximation in training has been largely unexplored. 

In this project we will focus on optimizing the training of deep neural networks from a systems perspective.
We will explore how approximation can help reduce training time without sacrificing accuracy. The project
is highly relevant to the course and would focus on a number of concepts covered in the course, such as
consistency and distributed computation models

