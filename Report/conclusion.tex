\section{Conclusion}
In this project, we studied the impact of using approximation in the form of training and update elisions for deep neural network training. Specifically, we have measured the impact of such approximation on classification accuracy and training speedup. We performed experiments by training a Convolutional Neural Network (CNN) model on the CIFAR-10 dataset using the TensorFlow framework. Two types of training elision techniques were considered, namely Selective Early Termination and Periodic Training Elision. For both these techniques we varied the number of layers for which updates were elided. We found that both of these techniques lead to significant speedup in training time with a minor impact on training accuracy.  Of the two techniques proposed, we observe that while the obtained accuracy is similar, Selective Early Termination shows potential for higher training speedup. Our experiments also indicate that the upper layers of our trained model are more resilient to approximations. However, eliding updates for upper layers does not provide significant speedup due to the expensive backpropogation step involved. Hence, we focussed on update elision for the lower layers of the network.
We leave it to future work to investigate a combination of both proposed techniques. Additionally, we believe that it would interesting to investigate techniques to dyamically or statically determine when and for which layers training and update elisions can be safely applied to enable scalable distributed training.  
