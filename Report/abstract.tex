\begin{abstract}

Increasing amounts of training data and computational resources have lead to machine learning models such as deep neural networks (DNNs) becoming practically feasible and more accurate. However, in order to train these DNNs in reasonable amounts of time, they require significant amounts of compute resources. This has motivated the need for large scale distributing training across many nodes. However, distribution at large scale poses a huge challenge as multiple workers need to synchronize and exchange data among themselves while training the model. In such situations, the network along with stragglers become a bottleneck for distributed training. To address this scalability bottleneck, we aim to leverage the tolerance of neural networks to approximations, thus targetting a reduction in both computation time and network bandwidth. Specifically, in this work, we aim to test the hypothesis that different layers in a neural network require different amounts of training. To this end, we explore different approximation strategies that elide updates to the different layers of the network. Our experiments reveal that eliding gradient computation and communication updates for a few layers of the neural network at opportune times can significantly speed up distributed training time without significantly compromising the accuracy of the trained model.
\end{abstract}



