\begin{abstract}

Increasing amounts of training data and computational resources have lead to many machine learning models becoming practically feasible and more accurate. Deep Neural Networks are one such technique that have gained wide acceptance in a variety of areas. However, in order to train them in reasonable amounts of time, they require significant amounts of compute resources. This has motivated the need for distributing training across multiple machines. However distribution at large scale poses a challenge, as multiple workers still need to coordinate among themselves while training the model. In such situations the network along with stragglers become the bottleneck for distributed training. To address this scalability bottleneck, we aim to leverage the tolerance of neural networks to approximations, thus targetting a reduction in both computation time and network bandwidth. Specifically, in this work, we aim to test the hypothesis that different layers in a neural network require different amounts of training. To this end, we explore different approximation strategies that elide updates to the different layers of the network. Our experiments reveal that eliding updates to a few layers of the neural network can significantly speed up distributed training time without significantly compromising the accuracy of the trained model.
\end{abstract}



