\section{Methodology}
\label{sec:methodology}
\subsection{Infrastructure}
We use PIN to perform our primary design analysis. PIN is dynamic binary instrumentation tool for
x86 architecture that collects runtime information by inserting extra code into
the program. PIN enables a much
faster evaluation without the full architecutural simulation. Since Gem5 is too
heavy-weight for initial experimentation, we built a
cache model with flexible coherence support using PIN. We added the "H" state
to our MESI-based cache simulator. We measure the difference in coherence
misses, invalidate/write back traffic and directory lookups to determine the
impact of the MESHI protocol. 

\begin{table}
  \centering
  \begin{tabular}{||c|c||}
	\hline
	
	\multicolumn{2}{||c||}{\textbf{Cache Hierarchy}} \\
	\hline
	L1 D-cache & 64KB/4 way, 64B block size\\ \hline
	Coherence Protocol & MESI Directory-based \\
	\hline

  \end{tabular}
  \caption{Simulator Parameters}
  \label{table:sim_param}		

\end{table}
\subsection{Assumptions and Simplifications}
We simply allow the approximate protocol to impact the cache performance for the
workloads in our evaluation. Impacting the correctness and precision of the
program as a result of our approximate cache protocol is beyond the scope of
this project. Since we cannot measure the impact on precision of the program, we
simply vary the \emph{degree of approximation} as opposed to implementing any
specific error bounding mechanism described above. We vary the degree of
approximation by controlling the fraction of writes that are treated as "Big" as
opposed to "Small" by the cache coherence protocol. The is entirely
probabilistic and does not depend on the data within the cache line, nor the
number of updates to any particular cache line. In that sense, the results
below depict the worst case. We believe that more intelligent "Big" vs "Small"
prediction mechanisms are likely to lead to better results. We, however, leave
this evaluation as part of future work. 

\subsection{Workloads}
We use a subset of benchmarks from the PARSEC-2.1 suite - namely
\emph{streamcluster, bodytrack, vips, fluidanimate, canneal}. We picked
benchmarks that showed the highest amount of data sharing with short running
times. We used the \emph{simmedium} input set for our evaluation. We programmed
our simulator to collect data within the Region of Interests within each
benchmark and assume that all data touched within the ROI to be approximable.



